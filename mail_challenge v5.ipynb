{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful fns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_callback(df, mail_id, sender, receiver, mid_best_cosine, likelihood, p1, p2, p3 ):\n",
    "    \"\"\"Function to save computed probabilities in dataframe df, during CV or Test computation\"\"\"\n",
    "    df.loc[-1] = [ mail_id, sender, receiver, mid_best_cosine, likelihood, p1, p2, p3]\n",
    "    df.index = df.index + 1\n",
    "    \n",
    "def is_csr_matrix_only_zeroes(my_csr_matrix):\n",
    "    \"\"\"Return True if a sparse matrix is composed only of zeros\"\"\"\n",
    "    return(len(my_csr_matrix.nonzero()[0]) == 0)\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def read_mail(mid, col_id = 'pre_processed'):\n",
    "    \"\"\" Based on a mail_id, returns its body or pre-processed body \"\"\"\n",
    "    if df_cv[df_cv['mid'] == int(mid)].empty == False:\n",
    "        out = df_cv[df_cv['mid'] == int(mid)][col_id].values[0]\n",
    "    elif df_train[df_train['mid'] == int(mid)].empty == False:\n",
    "        out = df_train[df_train['mid'] == int(mid)][col_id].values[0]\n",
    "    elif df_test[df_test['mid'] == int(mid)].empty == False:\n",
    "        out = df_test[df_test['mid'] == int(mid)][col_id].values[0]\n",
    "    else :\n",
    "        out = \"Mail non trouvé... \"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = \"Data/\"\n",
    "\n",
    "##########################\n",
    "# load some of the files #                           \n",
    "##########################\n",
    "\n",
    "training = pd.read_csv(path_to_data + 'training_set.csv', sep=',', header=0)\n",
    "\n",
    "training_info = pd.read_csv('training_info_processed.csv', sep=',', header=0)\n",
    "\n",
    "test_info = pd.read_csv('test_info_processed.csv', sep=',', header=0)\n",
    "\n",
    "test = pd.read_csv(path_to_data + 'test_set.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# create some handy structures #                    \n",
    "################################\n",
    "                            \n",
    "# convert training set to dictionary\n",
    "emails_ids_per_sender = {}\n",
    "for index, series in training.iterrows():\n",
    "    row = series.tolist()\n",
    "    sender = row[0]\n",
    "    ids = row[1:][0].split(' ')\n",
    "    emails_ids_per_sender[sender] = ids\n",
    "\n",
    "# save all unique sender names\n",
    "all_senders = emails_ids_per_sender.keys()\n",
    "\n",
    "# create address book with frequency information for each user\n",
    "address_books = {}\n",
    "i = 0\n",
    "\n",
    "\n",
    "if (os.path.isfile('all_users.pkl')) & (os.path.isfile('address_books.pkl')) & (os.path.isfile('all_recs.pkl')):\n",
    "    '''save files, so as to avoid computation each time the notebook is opened'''\n",
    "    all_recs = pickle.load(open('all_recs.pkl', 'rb'))                                   \n",
    "    all_users = pickle.load(open('all_users.pkl', 'rb'))\n",
    "    address_books = pickle.load(open('address_books.pkl', 'rb'))\n",
    "else:\n",
    "    for sender, ids in emails_ids_per_sender.items():\n",
    "        recs_temp = []\n",
    "        for my_id in ids:\n",
    "\n",
    "            '''Recipients'''\n",
    "            recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "            recipients = recipients[0].split(' ')\n",
    "            # keep only legitimate email addresses\n",
    "            recipients = [rec for rec in recipients if '@' in rec]\n",
    "            recs_temp.append(recipients)\n",
    "\n",
    "\n",
    "            '''mail info'''\n",
    "\n",
    "        # flatten    \n",
    "        recs_temp = [elt for sublist in recs_temp for elt in sublist]\n",
    "        # compute recipient counts\n",
    "        rec_occ = dict(Counter(recs_temp))\n",
    "        # order by frequency\n",
    "        sorted_rec_occ = sorted(rec_occ.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        # save\n",
    "        address_books[sender] = sorted_rec_occ\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print (i)\n",
    "        i += 1\n",
    "\n",
    "    # save all unique recipient names    \n",
    "    all_recs = list(set([elt[0] for sublist in address_books.values() for elt in sublist]))\n",
    "\n",
    "    # save all unique user names \n",
    "    all_users = []\n",
    "    all_users.extend(all_senders)\n",
    "    all_users.extend(all_recs)\n",
    "    all_users = list(set(all_users))\n",
    "    \n",
    "    pickle.dump(all_recs, open('all_recs.pkl', 'wb')) \n",
    "    pickle.dump(all_users, open('all_users.pkl', 'wb')) \n",
    "    pickle.dump(address_books, open('address_books.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DG and MG\n"
     ]
    }
   ],
   "source": [
    "'''Construct the communication graph of senders/receivers'''\n",
    "\n",
    "import networkx as nx\n",
    "import pdb\n",
    "\n",
    "DG_path = 'DG.text' # communication Graph\n",
    "MG_path = 'MG.text' # Graph giving more in-depth information on communications between sender and receiver\n",
    "\n",
    "if (os.path.isfile(DG_path)) & (os.path.isfile(MG_path)):\n",
    "    '''save files, so as to avoid computation each time the notebook is opened'''\n",
    "\n",
    "    DG = pickle.load(open(DG_path))\n",
    "    MG = pickle.load(open(MG_path))\n",
    "\n",
    "else:\n",
    "    DG=nx.DiGraph()\n",
    "    MG = nx.MultiDiGraph()\n",
    "\n",
    "    for sender, ids in emails_ids_per_sender.items():\n",
    "    #     recs_temp = []\n",
    "        DG.add_node(sender)\n",
    "        MG.add_node(sender)\n",
    "        recs_temp = []\n",
    "        recipients = []\n",
    "        for my_id in ids:\n",
    "            recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "            recipients = recipients[0].split(' ')\n",
    "            # keep only legitimate email addresses\n",
    "            recipients = [rec for rec in recipients if \"@\" in rec]\n",
    "\n",
    "            DG.add_nodes_from(recipients)\n",
    "            MG.add_nodes_from(recipients)\n",
    "\n",
    "            for recipient in recipients:\n",
    "                MG.add_edge(sender, recipient, email = my_id)\n",
    "                if DG.has_edge(sender, recipient):\n",
    "                    # we added this one before, just increase the weight by one\n",
    "                    DG[sender][recipient]['weight'] += 1\n",
    "                else:\n",
    "                    # new edge. add with weight=1\n",
    "                    DG.add_edge(sender, recipient, weight = 1)\n",
    "    '''saving graphs'''\n",
    "    print(\"Saving DG and MG\")\n",
    "    pickle.dump(DG, open('DG.txt', 'wb'))\n",
    "    pickle.dump(MG, open('MG.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing mail bodies with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # from nltk import FreqDist\n",
    "# big_string = training_info['pre_processed'].str.cat(sep=',')\n",
    "# fdistr = Counter(big_string.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/benlet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/benlet/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/benlet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/benlet/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Mail steming imports'''\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('maxent_treebank_pos_tagger') # for POS tagging\n",
    "nltk.download('stopwords') # stopwords\n",
    "nltk.download('averaged_perceptron_tagger') # stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process(content):\n",
    "    # Remove formatting\n",
    "    content =  re.sub(\"\\s+\", \" \", content)\n",
    "    # Convert to lower case\n",
    "    content = content.lower()\n",
    "    # Replace punctuation by space (preserving intra-word dashes)\n",
    "    content = \"\".join(letter if letter not in punct else \" \" for letter in content )\n",
    "    # Remove punctuation (preserving intra-word dashes)\n",
    "    content = \"\".join(letter for letter in content if letter not in punct)\n",
    "    # Remove extra white space\n",
    "    content = re.sub(\" +\",\" \", content)\n",
    "    # Remove leading and trailing white space\n",
    "    content = content.strip()\n",
    "    # Tokenize and stopword removal\n",
    "    tokens_keep  = [word for word in content.split() if word not in stpwds] \n",
    "    # POS-tag \n",
    "    tagged_tokens = nltk.pos_tag(tokens_keep)\n",
    "    # Keep only nouns and adjectives    \n",
    "    tokens_keep = [pair[0] for pair in tagged_tokens if (pair[1] in [\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\",\"JJS\",\"JJR\"])]\n",
    "    # Apply Porter stemmer\n",
    "    tokens_keep = [stemmer.stem(token) for token in tokens_keep]\n",
    "    return tokens_keep\n",
    "\n",
    "def pre_process_to_string(content):\n",
    "    return \",\".join(pre_process(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_all_body_column(df, new_col_name, test=False):\n",
    "    '''Process strings for all training and test bodies\n",
    "       This will create a new column on df with the pre-processing.\n",
    "    \n",
    "    '''\n",
    "    process = lambda x: pre_process_to_string(x)\n",
    "    df[new_col_name] = df['body'].apply(process)\n",
    "    if test :\n",
    "        df.to_csv(\"test_info_processed.csv\")\n",
    "    else :\n",
    "        df.to_csv(\"training_info_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Launch pre-processing'''\n",
    "# process_all_body_column(training_info, 'pre-processed')\n",
    "# process_all_body_column(test_info,'pre-processed', test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First names features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process(content, naming= False):\n",
    "    '''From a string, pre-process it to a list of tokens'''\n",
    "    \n",
    "    # Separate upper names in cases such as : Clement/BenoitAttached\n",
    "    content =  re.sub(\"\\s+\", \" \", content)\n",
    "\n",
    "#     Convert to lower case\n",
    "    content = content.lower()\n",
    "    \n",
    "    # Replace punctuation by space (preserving intra-word dashes)\n",
    "    content = \"\".join(letter if letter not in punct else \" \" for letter in content )\n",
    "    \n",
    "    # Remove punctuation (preserving intra-word dashes)\n",
    "    content = \"\".join(letter for letter in content if letter not in punct)\n",
    "    \n",
    "    # Remove extra white space\n",
    "    content = re.sub(\" +\",\" \", content)\n",
    "    # Remove leading and trailing white space\n",
    "    content = content.strip()\n",
    "    # Tokenize and stopword removal\n",
    "    tokens_keep  = [word for word in content.split() if word not in stpwds] \n",
    "    # POS-tag \n",
    "    tagged_tokens = nltk.pos_tag(tokens_keep)\n",
    "#     Keep only nouns \n",
    "    tokens_keep = [pair[0] for pair in tagged_tokens if (pair[1] in [\"NNP\",\"NNPS\"])]\n",
    "    \n",
    "    # Remove extra white space\n",
    "    tokens_keep = [content.lower() for content in tokens_keep]\n",
    "    return tokens_keep\n",
    "\n",
    "def pre_process_to_string(content):\n",
    "    return \",\".join(pre_process(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Launch pre-processing for names'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Launch pre-processing for names'''\n",
    "# process_all_body_column(training_info, 'NNP')\n",
    "# process_all_body_column(test_info,'NNP', test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_mails_address_to_names(mail_address):\n",
    "    '''from email (eg : 'amber.keenan@enron.com' ) returns ['amber', 'keenan'] '''\n",
    "    name_mail = \"\".join(letter if letter not in \"@\" else \" \" for letter in mail_address).split(\" \")[0]\n",
    "    name_mail = \"\".join(letter if letter not in punct else \",\" for letter in name_mail)\n",
    "    \n",
    "    return name_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amber', 'keenan']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_mails_address_to_names('amber.keenan@enron.com').split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting TF-IDF features for emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_info.fillna(\"\", inplace = True)\n",
    "test_info.fillna(\"\", inplace = True)\n",
    "nb_training = training_info.shape[0]\n",
    "nb_test = test_info.shape[0]\n",
    "\n",
    "# create a string with all mails for TF-IDF feature extractor\n",
    "all_mails = pd.concat([training_info['pre_processed'], test_info['pre_processed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45975, 118013)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TF-IDF on emails\n",
    "   We use TF-IDF extractor from sklearn\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all_mails = vectorizer.fit_transform(all_mails)\n",
    "X_all_mails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Creating CV set'''\n",
    "# we seed our random number as we want to have reproducible results\n",
    "np.random.seed(9001)\n",
    "mask = np.random.permutation(nb_training)\n",
    "train_id = mask[:int(0.99*nb_training)]\n",
    "cv_id = mask[int(0.99*nb_training):]\n",
    "X_mails_train = X_all_mails[train_id, :]\n",
    "X_mails_cv = X_all_mails[cv_id, :]\n",
    "X_mails_test = X_all_mails[nb_training:, :]\n",
    "df_train = training_info.iloc[train_id].reset_index()\n",
    "df_cv = training_info.iloc[cv_id].reset_index()\n",
    "df_test = test_info.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_sparse(X1, X2, idx1, idx2):\n",
    "    '''return cosine similarity between two TFIDF representation of two emails'''\n",
    "    if is_csr_matrix_only_zeroes(X1[idx1]) or is_csr_matrix_only_zeroes(X2[idx2]):\n",
    "        out = 0\n",
    "    else : \n",
    "        out = cosine_similarity(X1[idx1], X2[idx2])[0][0]\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing likelihoods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 _ Sender Likelihood P(S/R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def total_incoming_mails(receiver_):\n",
    "    return sum([DG[sender_][receiver_]['weight'] \\\n",
    "                for sender_ in DG.predecessors(receiver_)])\n",
    "\n",
    "# dictionnary of incoming mails per receiver \n",
    "dict_incoming_mails = {}\n",
    "for recipient in all_recs:\n",
    "    dict_incoming_mails[recipient] = total_incoming_mails(recipient)\n",
    "    \n",
    "\n",
    "def p_fred_S_sachant_R(DG, sender, receiver, k=k):\n",
    "    \n",
    "    out = DG[sender][receiver]['weight']/ \\\n",
    "                dict_incoming_mails[receiver]\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "sender = 'sylvia.hu@enron.com'\n",
    "receiver = 'britt.davis@enron.com'\n",
    "p_fred_S_sachant_R(DG=DG, sender=sender, receiver=receiver, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 _ Recipient Likelihood P(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Recipient Likelihood'''\n",
    "\n",
    "# number of emails received by Receiver / Total # of emails sent\n",
    "\n",
    "def total_mail_sent(DG=DG):\n",
    "    A = np.array(list(DG.edges_iter(data='weight', default=1)))\n",
    "    return np.sum(A[:,2:].flatten().astype(np.int),axis=0)\n",
    "\n",
    "Total_emails_sent = total_mail_sent()\n",
    "\n",
    "def p_R(DG, receiver):\n",
    "    out = dict_incoming_mails[receiver] /Total_emails_sent\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 576 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.2967627231646784e-05"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "p_R(DG=DG, receiver= receiver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 _ Email Likelihood P(E/R,S) - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Email likelihood with TF_IDF'''\n",
    "\n",
    "def p_e_sachant_r_s_tfidf(email_id, sender, receiver, cv_or_test= 'cv'):\n",
    "    '''\n",
    "    For a given email, the sender of this emails and potential recipients :  \n",
    "    returns :  \n",
    "        - the maximum cosine_similarity between the given email and all the emails between sender & receiver\n",
    "        - Mail ID for this maximum cosine similarity '''\n",
    "    out = 1e-15\n",
    "    best_cosine = None\n",
    "    if MG.get_edge_data(sender,receiver): \n",
    "        '''list all mails between sender and receiver'''\n",
    "        mail_list = [ a for sublist in \\\n",
    "                     [list(s.values()) for s in MG.get_edge_data(sender,receiver).values()] for a in sublist  ]\n",
    "        \n",
    "        mail_list = [mid for mid in mail_list if df_cv[df_cv['mid'] == int(mid)].empty == True  ]\n",
    "        \n",
    "        if mail_list:\n",
    "            mail_tf_idf_scores = []\n",
    " \n",
    "            for mid in mail_list:\n",
    "                idx_train = df_train[df_train['mid'] == int(mid)].index.values[0]\n",
    "                if cv_or_test == 'test':\n",
    "                    idx_test = df_test[df_test['mid'] == int(email_id)].index.values[0]\n",
    "                    mail_tf_idf_scores.append( cosine_sparse(X_mails_train, X_mails_test, idx_train, idx_test ) )\n",
    "                else :\n",
    "                    idx_cv = df_cv[df_cv['mid'] == int(email_id)].index.values[0]\n",
    "                    mail_tf_idf_scores.append( cosine_sparse(X_mails_train, X_mails_cv, idx_train, idx_cv ) )\n",
    "\n",
    "            if mail_tf_idf_scores:\n",
    "                out = np.max(np.array(mail_tf_idf_scores))\n",
    "                best_cosine = mail_list[np.argmax(np.array(mail_tf_idf_scores))]\n",
    "                if out ==0 :\n",
    "                    out = 1e-16\n",
    "    return out , best_cosine\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# p_e_sachant_r_s_tfidf(email_id, sender, receiver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the probabilistic model on CV Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# df_proba_cv =pd.DataFrame(columns=[\"mail_id\",\"sender\", \"receiver\",\"mid_best_cosine\", \"likelihood\",'P(E/R,S)', 'P(S/E)', 'P(R)'])\n",
    "# start_time = time.time()\n",
    "# print(\"Starting\")\n",
    "# for index, row in df_cv.iterrows():\n",
    "#     mail_id = df_cv.loc[index]['mid']\n",
    "#     sender = training[training['mids'].str.contains(str(mail_id))]['sender'].values[0]\n",
    "    \n",
    "#     receiver_count = 0\n",
    "#     for (receiver, _) in address_books[sender][:250]:\n",
    "        \n",
    "\n",
    "#         a2 = p_fred_S_sachant_R(DG, sender, receiver)\n",
    "#         a3 = p_R(DG, receiver)\n",
    "#         a1, mid_best_cosine = p_e_sachant_r_s_tfidf(mail_id,sender,receiver)\n",
    "#         out = a1 * a2 * a3 \n",
    "#         save_callback( df_proba_cv, mail_id, sender, receiver, mid_best_cosine, out, a1, a2, a3)\n",
    "#     receiver_count += 1\n",
    "#     print (\"CV Mail %d over %d calculated in %d min\" % (index, len(df_cv), (time.time() - start_time)/60 ))\n",
    "\n",
    "# df_proba_cv.to_csv(\"df_proba_cv.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_proba_cv = pd.read_csv(\"df_proba_cv.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_receivers = 10\n",
    "def f(weights = [1,1,1]):\n",
    "    mapk_predicted =[]\n",
    "    mapk_true = []\n",
    "    mapk_index = []\n",
    "    weights_serie = pd.Series(weights)\n",
    "    prd_scalar = lambda x: np.asarray(x) * np.asarray(weights_serie)\n",
    "    for index, row in df_cv[:20].iterrows():\n",
    "        mid_cv = row[4]\n",
    "        \n",
    "        selected_columns = df_proba_cv[df_proba_cv['mail_id'] == int(mid_cv)][['P(E/R,S)', 'P(S/E)', 'P(R)']]\n",
    "        loglikelihood = - np.log(selected_columns)\n",
    "        loglikelihood = loglikelihood.apply(prd_scalar, axis=1)\n",
    "        \n",
    "        loglikelihood['likelihood'] = loglikelihood.sum(axis = 1)\n",
    "        loglikelihood = loglikelihood.sort_values(by='likelihood', ascending = True)\n",
    "        receiver_names = df_proba_cv.loc[loglikelihood.index][:top_receivers]['receiver'].values\n",
    "        receiver_list_predicted = receiver_names\n",
    "\n",
    "        receiver_list_true = training_info[training_info['mid'] ==mid_cv]['recipients'].values[0].split(\" \")\n",
    "        mapk_index.append(mid_cv)\n",
    "        mapk_predicted.append(receiver_list_predicted)\n",
    "        mapk_true.append(receiver_list_true)\n",
    "    \n",
    "    return -mapk(mapk_true, mapk_predicted, k=10) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "\n",
    "# weights_ini  = [1, 1, 1]\n",
    "# best_weights = minimize(f, weights_ini, method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.546458333333\n",
      "[ 1.16822131  0.81410608  0.83203018]\n"
     ]
    }
   ],
   "source": [
    "print(- f(best_weights['x']))\n",
    "print (best_weights['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50493055555555555"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- f([1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "df_proba_test =pd.DataFrame(columns=[\"mail_id\",\"sender\", \"receiver\",\"mid_best_cosine\", \"likelihood\",'P(E/R,S)', 'P(S/E)', 'P(R)'])\n",
    "start_time = time.time()\n",
    "print(\"Starting\")\n",
    "for index, row in test.iterrows():\n",
    "    name_ids = row.tolist()\n",
    "    sender = name_ids[0]\n",
    "    # get IDs of the emails for which recipient prediction is needed\n",
    "    mids_predict = name_ids[1].split(' ')\n",
    "    mids_predict = [int(my_id) for my_id in mids_predict]\n",
    "    \n",
    "    receiver_count = 0\n",
    "    for (receiver, _) in address_books[sender][:250]:\n",
    "        for mail_id in mids_predict:\n",
    "            \n",
    "            a2 = p_fred_S_sachant_R(DG, sender, receiver)\n",
    "            a3 = p_R(DG, receiver)\n",
    "            a1, mid_best_cosine = p_e_sachant_r_s_tfidf(mail_id,sender,receiver, 'test')\n",
    "            out = a1 * a2 * a3 \n",
    "            save_callback( df_proba_test, mail_id, sender, receiver, mid_best_cosine, out, a1, a2, a3)\n",
    "        receiver_count += 1\n",
    "    print (\"Test Sender %d over %d calculated in %d min\" % (index, len(df_test), (time.time() - start_time)/60 ))\n",
    "\n",
    "df_proba_test.to_csv(\"df_proba_test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_proba_test = pd.read_csv('df_proba_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing of addresses with '..' inside\n",
    "ix_to_false = df_proba_test[ (df_proba_test['P(S/E)'] == True) & (df_proba_test['receiver'].str.contains(str(\"\\.\\.\")))].index\n",
    "df_proba_test.loc[ix_to_false, 'P(S/E)'] = False\n",
    "df_proba_test['name_at_beginning'].replace(False, 0, inplace= True)\n",
    "df_proba_test['name_in_body'].replace(False, 0, inplace= True)\n",
    "df_proba_test['name_in_body'].replace(True, 1, inplace= True)\n",
    "df_proba_test['name_in_body'].replace(True, 1, inplace= True)\n",
    "\n",
    "df_proba = df_proba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiplylikelihood(weights = [1,1,1]):\n",
    "    '''This function allows to fine tune the weights applied to the test'''\n",
    "    df_pow = df_proba[['P(E/R,S)', 'P(S/E)', 'P(R)']].pow(weights)\n",
    "    \n",
    "    df_proba['likelihood'] = df_pow.prod(axis = 1)\n",
    "    \n",
    "    df_proba['likelihood'] = df_proba['likelihood'] + df_proba['name_at_beginning'] + df_proba['name_in_body']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Best weights found in CV'''\n",
    "multiplylikelihood(weights = [ 1.16,  0.81, 0.83])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[(df_proba['proba_id'] == 'p(R/S,E)') & (df_proba['sender'] =='ginger.dernehl@enron.com')].sort_values(by= 'likelihood' , ascending = False)\n",
    "df_proba = df_proba.drop(df_proba[(df_proba['receiver'] == df_proba['sender']) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path_to_data + 'predictions_random.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in submission.iterrows():\n",
    "    mail_id = row[0]\n",
    "    receivers_id = df_proba[df_proba['mail_id'] == mail_id].\\\n",
    "        sort_values(['likelihood'], ascending = False)[:10]['receiver'].values\n",
    "    receiver_list = \" \".join(receivers_id)\n",
    "    \n",
    "    submission.loc[index, 'recipients'] = receiver_list\n",
    "submission.to_csv('submission_test.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def multiplylikelihood(weights = [1,1,1,1,1]):\n",
    "    df_pow = df_proba[['P(E/R,S)', 'P(S/E)', 'P(R)','name_at_beginning', 'name_in_body']].pow(weights)\n",
    "    \n",
    "    df_proba['likelihood'] = df_pow.prod(axis = 1)\n",
    "    \n",
    "# df_proba[(df_proba['proba_id'] == 'p(R/S,E)') & (df_proba['sender'] =='ginger.dernehl@enron.com')].sort_values(by= 'likelihood' , ascending = False)\n",
    "df_proba = df_proba.drop(df_proba[(df_proba['receiver'] == df_proba['sender']) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
