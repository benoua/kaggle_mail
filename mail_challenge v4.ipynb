{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful fns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_callback(df, mail_id, sender, receiver, mid_best_cosine, likelihood, p1, p2, p3 ):\n",
    "    \n",
    "    df.loc[-1] = [ mail_id, sender, receiver, mid_best_cosine, likelihood, p1, p2, p3]\n",
    "    df.index = df.index + 1\n",
    "    \n",
    "def is_csr_matrix_only_zeroes(my_csr_matrix):\n",
    "    return(len(my_csr_matrix.nonzero()[0]) == 0)\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def read_mail(mid, pre_processed = 'pre_processed'):\n",
    "    if df_cv[df_cv['mid'] == int(mid)].empty == False:\n",
    "        out = df_cv[df_cv['mid'] == int(mid)][pre_processed].values[0]\n",
    "    elif df_train[df_train['mid'] == int(mid)].empty == False:\n",
    "        out = df_train[df_train['mid'] == int(mid)][pre_processed].values[0]\n",
    "    elif df_test[df_test['mid'] == int(mid)].empty == False:\n",
    "        out = df_test[df_test['mid'] == int(mid)][pre_processed].values[0]\n",
    "    else :\n",
    "        out = \"Mail non trouv√©... \"\n",
    "    print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mail 110806\n",
    "# # 93999    1.0\n",
    "# read_mail(110806, 'body')\n",
    "# read_mail(181457, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk([list(np.arange(10))], [list( np.random.permutation(10))], k=10)\n",
    "mapk([['a', 'b']], [['b', 'c', 'd']], k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = \"Data/\"\n",
    "\n",
    "##########################\n",
    "# load some of the files #                           \n",
    "##########################\n",
    "\n",
    "training = pd.read_csv(path_to_data + 'training_set.csv', sep=',', header=0)\n",
    "\n",
    "training_info = pd.read_csv('training_info_processed.csv', sep=',', header=0)\n",
    "\n",
    "test_info = pd.read_csv('test_info_processed.csv', sep=',', header=0)\n",
    "\n",
    "test = pd.read_csv(path_to_data + 'test_set.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes :\n",
    "Careful, there is duplicates in 'mid' for training_info and test_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# create some handy structures #                    \n",
    "################################\n",
    "                            \n",
    "# convert training set to dictionary\n",
    "emails_ids_per_sender = {}\n",
    "for index, series in training.iterrows():\n",
    "    row = series.tolist()\n",
    "    sender = row[0]\n",
    "    ids = row[1:][0].split(' ')\n",
    "    emails_ids_per_sender[sender] = ids\n",
    "\n",
    "# save all unique sender names\n",
    "all_senders = emails_ids_per_sender.keys()\n",
    "\n",
    "# create address book with frequency information for each user\n",
    "address_books = {}\n",
    "i = 0\n",
    "\n",
    "if (os.path.isfile('all_users.pkl')) & (os.path.isfile('address_books.pkl')) & (os.path.isfile('all_recs.pkl')):\n",
    "    all_recs = pickle.load(open('all_recs.pkl', 'rb'))                                   \n",
    "    all_users = pickle.load(open('all_users.pkl', 'rb'))\n",
    "    address_books = pickle.load(open('address_books.pkl', 'rb'))\n",
    "else:\n",
    "    for sender, ids in emails_ids_per_sender.items():\n",
    "        recs_temp = []\n",
    "        for my_id in ids:\n",
    "\n",
    "            '''Recipients'''\n",
    "            recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "            recipients = recipients[0].split(' ')\n",
    "            # keep only legitimate email addresses\n",
    "            recipients = [rec for rec in recipients if '@' in rec]\n",
    "            recs_temp.append(recipients)\n",
    "\n",
    "\n",
    "            '''mail info'''\n",
    "\n",
    "        # flatten    \n",
    "        recs_temp = [elt for sublist in recs_temp for elt in sublist]\n",
    "        # compute recipient counts\n",
    "        rec_occ = dict(Counter(recs_temp))\n",
    "        # order by frequency\n",
    "        sorted_rec_occ = sorted(rec_occ.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        # save\n",
    "        address_books[sender] = sorted_rec_occ\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print (i)\n",
    "        i += 1\n",
    "\n",
    "    # save all unique recipient names    \n",
    "    all_recs = list(set([elt[0] for sublist in address_books.values() for elt in sublist]))\n",
    "\n",
    "    # save all unique user names \n",
    "    all_users = []\n",
    "    all_users.extend(all_senders)\n",
    "    all_users.extend(all_recs)\n",
    "    all_users = list(set(all_users))\n",
    "    \n",
    "    pickle.dump(all_recs, open('all_recs.pkl', 'wb')) \n",
    "    pickle.dump(all_users, open('all_users.pkl', 'wb')) \n",
    "    pickle.dump(address_books, open('address_books.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DG and MG\n"
     ]
    }
   ],
   "source": [
    "'''Construct the communication graph of senders/receivers'''\n",
    "\n",
    "import networkx as nx\n",
    "import pdb\n",
    "\n",
    "DG_path = 'DG.text'\n",
    "MG_path = 'MG.text'\n",
    "\n",
    "# check if it already exists\n",
    "if (os.path.isfile(DG_path)) & (os.path.isfile(MG_path)):\n",
    "    DG = pickle.load(open(DG_path))\n",
    "    MG = pickle.load(open(MG_path))\n",
    "\n",
    "else:\n",
    "    DG=nx.DiGraph()\n",
    "    MG = nx.MultiDiGraph()\n",
    "\n",
    "    for sender, ids in emails_ids_per_sender.items():\n",
    "    #     recs_temp = []\n",
    "        DG.add_node(sender)\n",
    "        MG.add_node(sender)\n",
    "        recs_temp = []\n",
    "        recipients = []\n",
    "        for my_id in ids:\n",
    "            recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "            recipients = recipients[0].split(' ')\n",
    "            # keep only legitimate email addresses\n",
    "            recipients = [rec for rec in recipients if \"@\" in rec]\n",
    "\n",
    "            DG.add_nodes_from(recipients)\n",
    "            MG.add_nodes_from(recipients)\n",
    "\n",
    "            for recipient in recipients:\n",
    "                MG.add_edge(sender, recipient, email = my_id)\n",
    "                if DG.has_edge(sender, recipient):\n",
    "                    # we added this one before, just increase the weight by one\n",
    "                    DG[sender][recipient]['weight'] += 1\n",
    "                else:\n",
    "                    # new edge. add with weight=1\n",
    "                    DG.add_edge(sender, recipient, weight = 1)\n",
    "    '''saving graphs'''\n",
    "    print(\"Saving DG and MG\")\n",
    "    pickle.dump(DG, open('DG.txt', 'wb'))\n",
    "    pickle.dump(MG, open('MG.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # from nltk import FreqDist\n",
    "big_string = training_info['pre_processed'].str.cat(sep=',')\n",
    "fdistr = Counter(big_string.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_all_body_column(training_info, test=False):\n",
    "    process = lambda x: pre_process_to_string(x)\n",
    "    training_info['NNP'] = training_info['body'].apply(process)\n",
    "    if test :\n",
    "        training_info.to_csv(\"test_info_processed.csv\")\n",
    "    else :\n",
    "        training_info.to_csv(\"training_info_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/benlet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/benlet/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/benlet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/benlet/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "'''Mail steming'''\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('maxent_treebank_pos_tagger') # for POS tagging\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nameparser.parser import HumanName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_mails_address_to_names(mail_address):\n",
    "    name_mail = \"\".join(letter if letter not in \"@\" else \" \" for letter in mail_address).split(\" \")[0]\n",
    "    name_mail = \"\".join(letter if letter not in punct else \",\" for letter in name_mail)\n",
    "    \n",
    "    return name_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amber', 'keenan']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_mails_address_to_names('amber.keenan@enron.com').split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process(content):\n",
    "    # Remove formatting\n",
    "    content =  re.sub(\"\\s+\", \" \", content)\n",
    "    # Convert to lower case\n",
    "#     content = content.lower()\n",
    "    \n",
    "    # Replace punctuation by space (preserving intra-word dashes)\n",
    "    content = \"\".join(letter if letter not in punct else \" \" for letter in content )\n",
    "    \n",
    "    # Remove punctuation (preserving intra-word dashes)\n",
    "    content = \"\".join(letter for letter in content if letter not in punct)\n",
    "    \n",
    "    # Remove extra white space\n",
    "    content = re.sub(\" +\",\" \", content)\n",
    "    # Remove leading and trailing white space\n",
    "    content = content.strip()\n",
    "    # Tokenize and stopword removal\n",
    "    tokens_keep  = [word for word in content.split() if word not in stpwds] \n",
    "    # POS-tag \n",
    "    tagged_tokens = nltk.pos_tag(tokens_keep)\n",
    "#     Keep only nouns and adjectives    \n",
    "    tokens_keep = [pair[0] for pair in tagged_tokens if (pair[1] in [\"NNP\",\"NNPS\"])]\n",
    "                                                         # [\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\",\"JJS\",\"JJR\"])]\n",
    "    \n",
    "    # Remove extra white space\n",
    "    tokens_keep = [content.lower() for content in tokens_keep]\n",
    "    # Apply Porter stemmer\n",
    "    tokens_keep = [stemmer.stem(token) for token in tokens_keep]\n",
    "    return tokens_keep\n",
    "\n",
    "def pre_process_to_string(content):\n",
    "    return \",\".join(pre_process(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_all_body_column(training_info)\n",
    "process_all_body_column(test_info, test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kevin,bob,con,legal'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process_to_string(\"Kevin/Bob: Here is a quick rundown on the Cons Legal has been assessing the risks of doing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_info.fillna(\"\", inplace = True)\n",
    "test_info.fillna(\"\", inplace = True)\n",
    "nb_training = training_info.shape[0]\n",
    "nb_test = test_info.shape[0]\n",
    "all_mails = pd.concat([training_info['pre_processed'], test_info['pre_processed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45975, 118013)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TF-IDF on emails'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all_mails = vectorizer.fit_transform(all_mails)\n",
    "X_all_mails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33163 23917 32382 ..., 32286 21413 43065]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(9001)\n",
    "mask = np.random.permutation(nb_training)\n",
    "train_id = mask[:int(0.99*nb_training)]\n",
    "cv_id = mask[int(0.99*nb_training):]\n",
    "print (mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_mails_train = X_all_mails[train_id, :]\n",
    "X_mails_cv = X_all_mails[cv_id, :]\n",
    "X_mails_test = X_all_mails[nb_training:, :]\n",
    "df_train = training_info.iloc[train_id].reset_index()\n",
    "df_cv = training_info.iloc[cv_id].reset_index()\n",
    "df_test = test_info.reset_index()\n",
    "len(df_cv) == X_mails_cv.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cosine_df(df1, df2, idx1, idx2):\n",
    "    return cosine_similarity(np.atleast_2d(df1.loc[idx1].values), np.atleast_2d(df2.loc[idx2].values))[0][0]\n",
    "\n",
    "def cosine_sparse(X1, X2, idx1, idx2):\n",
    "    if is_csr_matrix_only_zeroes(X1[idx1]) or is_csr_matrix_only_zeroes(X2[idx2]):\n",
    "        out = 0\n",
    "    else : \n",
    "        out = cosine_similarity(X1[idx1], X2[idx2])[0][0]\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing likelihoods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Sender Likelihood'''\n",
    "k = 10\n",
    "# frequency based probability\n",
    "sender = 'sylvia.hu@enron.com'\n",
    "receiver = 'britt.davis@enron.com'\n",
    "\n",
    "def total_incoming_mails(receiver_):\n",
    "    return sum([DG[sender_][receiver_]['weight'] \\\n",
    "                for sender_ in DG.predecessors(receiver_)])\n",
    "\n",
    "# dictionnary of incoming mails per receiver \n",
    "dict_incoming_mails = {}\n",
    "for recipient in all_recs:\n",
    "    dict_incoming_mails[recipient] = total_incoming_mails(recipient)\n",
    "    \n",
    "\n",
    "def p_fred_S_sachant_R(DG, sender, receiver, k=k):\n",
    "    \n",
    "    out = DG[sender][receiver]['weight']/ \\\n",
    "                dict_incoming_mails[receiver]\n",
    "    \n",
    "#     save_callback(\"p(S/R)\", None, sender, receiver, None, out )\n",
    "    return out\n",
    "    \n",
    "# co-occurence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_fred_S_sachant_R(DG=DG, sender=sender, receiver=receiver, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Recipient Likelihood'''\n",
    "\n",
    "# number of emails received by Receiver / Total # of emails sent\n",
    "\n",
    "def total_mail_sent(DG=DG):\n",
    "    A = np.array(list(DG.edges_iter(data='weight', default=1)))\n",
    "    return np.sum(A[:,2:].flatten().astype(np.int),axis=0)\n",
    "\n",
    "Total_emails_sent = total_mail_sent()\n",
    "\n",
    "def p_R(DG, receiver):\n",
    "    out = dict_incoming_mails[receiver] /Total_emails_sent\n",
    "#     save_callback(\"p(R)\", None, None, receiver, None, out )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 18.8 ¬µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.2967627231646784e-05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "p_R(DG=DG, receiver= receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Email likelihood with TF_IDF'''\n",
    "\n",
    "\n",
    "def p_e_sachant_r_s_tfidf(email_id, sender, receiver, cv_or_test= 'cv'):\n",
    "    '''for a given email, the sender of this emails and potential recipients, returns \n",
    "    - the maximum cosine_similarity between the given email and all the emails between sender & receiver'''\n",
    "    out = 1e-15\n",
    "    best_cosine = None\n",
    "    if MG.get_edge_data(sender,receiver): \n",
    "        '''list all mails between sender and receiver'''\n",
    "        mail_list = [ a for sublist in \\\n",
    "                     [list(s.values()) for s in MG.get_edge_data(sender,receiver).values()] for a in sublist  ]\n",
    "        \n",
    "        mail_list = [mid for mid in mail_list if df_cv[df_cv['mid'] == int(mid)].empty == True  ]\n",
    "        \n",
    "        if mail_list:\n",
    "            mail_tf_idf_scores = []\n",
    " \n",
    "            for mid in mail_list:\n",
    "                idx_train = df_train[df_train['mid'] == int(mid)].index.values[0]\n",
    "                if cv_or_test == 'test':\n",
    "                    idx_test = df_test[df_test['mid'] == int(email_id)].index.values[0]\n",
    "                    mail_tf_idf_scores.append( cosine_sparse(X_mails_train, X_mails_test, idx_train, idx_test ) )\n",
    "                else :\n",
    "                    idx_cv = df_cv[df_cv['mid'] == int(email_id)].index.values[0]\n",
    "                    mail_tf_idf_scores.append( cosine_sparse(X_mails_train, X_mails_cv, idx_train, idx_cv ) )\n",
    "\n",
    "            if mail_tf_idf_scores:\n",
    "                out = np.max(np.array(mail_tf_idf_scores))\n",
    "                best_cosine = mail_list[np.argmax(np.array(mail_tf_idf_scores))]\n",
    "                if out ==0 :\n",
    "                    out = 1e-16\n",
    "#                 save_callback(\"p(E/S,R)\", email_id, sender, receiver, None, out)\n",
    "    return out , best_cosine\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.25 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# p_e_sachant_r_s_tfidf(email_id, sender, receiver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the probabilistic model on CV Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# df_proba_cv =pd.DataFrame(columns=[\"mail_id\",\"sender\", \"receiver\",\"mid_best_cosine\", \"likelihood\",'P(E/R,S)', 'P(S/E)', 'P(R)'])\n",
    "# start_time = time.time()\n",
    "# print(\"Starting\")\n",
    "# for index, row in df_cv.iterrows():\n",
    "#     mail_id = df_cv.loc[index]['mid']\n",
    "#     sender = training[training['mids'].str.contains(str(mail_id))]['sender'].values[0]\n",
    "    \n",
    "#     receiver_count = 0\n",
    "#     for (receiver, _) in address_books[sender][:250]:\n",
    "        \n",
    "\n",
    "#         a2 = p_fred_S_sachant_R(DG, sender, receiver)\n",
    "#         a3 = p_R(DG, receiver)\n",
    "#         a1, mid_best_cosine = p_e_sachant_r_s_tfidf(mail_id,sender,receiver)\n",
    "#         out = a1 * a2 * a3 \n",
    "#         save_callback( df_proba_cv, mail_id, sender, receiver, mid_best_cosine, out, a1, a2, a3)\n",
    "#     receiver_count += 1\n",
    "#     print (\"CV Mail %d over %d calculated in %d min\" % (index, len(df_cv), (time.time() - start_time)/60 ))\n",
    "\n",
    "# df_proba_cv.to_csv(\"df_proba_cv.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_proba_cv = pd.read_csv(\"df_proba_cv.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_receivers = 10\n",
    "def f(weights = [1,1,1]):\n",
    "    mapk_predicted =[]\n",
    "    mapk_true = []\n",
    "    mapk_index = []\n",
    "    weights_serie = pd.Series(weights)\n",
    "    prd_scalar = lambda x: np.asarray(x) * np.asarray(weights_serie)\n",
    "    for index, row in df_cv[:20].iterrows():\n",
    "#         print(row[0])\n",
    "        mid_cv = row[3]\n",
    "#         print(\"mail %d\" %mid_cv)\n",
    "#         print (df_proba_cv[df_proba_cv['mail_id'] == mid_cv].sort_values(by='P(E/R,S)', ascending = False)[:5]['P(E/R,S)'])\n",
    "#         df_pow = df_proba_cv[df_proba_cv['mail_id'] == int(mid_cv)][['P(E/R,S)', 'P(S/E)', 'P(R)']].pow(weights)\n",
    "        \n",
    "        selected_columns = df_proba_cv[df_proba_cv['mail_id'] == int(mid_cv)][['P(E/R,S)', 'P(S/E)', 'P(R)']]\n",
    "        loglikelihood = - np.log(selected_columns)\n",
    "        loglikelihood = loglikelihood.apply(prd_scalar, axis=1)\n",
    "        \n",
    "        loglikelihood['likelihood'] = loglikelihood.sum(axis = 1)\n",
    "        loglikelihood = loglikelihood.sort_values(by='likelihood', ascending = True)\n",
    "        receiver_names = df_proba_cv.loc[loglikelihood.index][:top_receivers]['receiver'].values\n",
    "    #     receiver_list_predicted = \" \".join(receiver_names)\n",
    "        receiver_list_predicted = receiver_names\n",
    "\n",
    "        receiver_list_true = training_info[training_info['mid'] ==mid_cv]['recipients'].values[0].split(\" \")\n",
    "#         print(receiver_list_predicted[:top_receivers])\n",
    "#         print(\"######\")\n",
    "#         print(receiver_list_true[:top_receivers])\n",
    "#         print(apk(receiver_list_true,receiver_list_predicted, k=10))\n",
    "        mapk_index.append(mid_cv)\n",
    "        mapk_predicted.append(receiver_list_predicted)\n",
    "        mapk_true.append(receiver_list_true)\n",
    "    \n",
    "    return -mapk(mapk_true, mapk_predicted, k=10) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "weights_ini  = [1, 1, 1]\n",
    "best_weights = minimize(f, weights_ini, method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.546458333333\n",
      "[ 1.16822131  0.81410608  0.83203018]\n"
     ]
    }
   ],
   "source": [
    "print(- f(best_weights['x']))\n",
    "print (best_weights['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50493055555555555"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- f([1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# df_proba_test =pd.DataFrame(columns=[\"mail_id\",\"sender\", \"receiver\",\"mid_best_cosine\", \"likelihood\",'P(E/R,S)', 'P(S/E)', 'P(R)'])\n",
    "# start_time = time.time()\n",
    "# print(\"Starting\")\n",
    "# for index, row in test.iterrows():\n",
    "#     name_ids = row.tolist()\n",
    "#     sender = name_ids[0]\n",
    "#     # get IDs of the emails for which recipient prediction is needed\n",
    "#     mids_predict = name_ids[1].split(' ')\n",
    "#     mids_predict = [int(my_id) for my_id in mids_predict]\n",
    "    \n",
    "#     receiver_count = 0\n",
    "#     for (receiver, _) in address_books[sender][:250]:\n",
    "#         for mail_id in mids_predict:\n",
    "            \n",
    "#             a2 = p_fred_S_sachant_R(DG, sender, receiver)\n",
    "#             a3 = p_R(DG, receiver)\n",
    "#             a1, mid_best_cosine = p_e_sachant_r_s_tfidf(mail_id,sender,receiver, 'test')\n",
    "#             out = a1 * a2 * a3 \n",
    "#             save_callback( df_proba_test, mail_id, sender, receiver, mid_best_cosine, out, a1, a2, a3)\n",
    "#         receiver_count += 1\n",
    "#     print (\"Test Sender %d over %d calculated in %d min\" % (index, len(df_test), (time.time() - start_time)/60 ))\n",
    "\n",
    "# df_proba_test.to_csv(\"df_proba_test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_proba = pd.read_csv('df_proba_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "# sound_file = 'reveil.mp3'\n",
    "# Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiplylikelihood(weights = [1,1,1]):\n",
    "    df_pow = df_proba[['P(E/R,S)', 'P(S/E)', 'P(R)']].pow(weights)\n",
    "    \n",
    "    df_proba['likelihood'] = df_pow.prod(axis = 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiplylikelihood(weights = [ 1.16,  0.81, 0.83])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[(df_proba['proba_id'] == 'p(R/S,E)') & (df_proba['sender'] =='ginger.dernehl@enron.com')].sort_values(by= 'likelihood' , ascending = False)\n",
    "df_proba = df_proba.drop(df_proba[(df_proba['receiver'] == df_proba['sender']) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[df_proba['mail_id'] == 298389.0].sort_values(['likelihood'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[(df_proba['mail_id'] == 298389)  \\\n",
    "# #         & (df_proba['sender'] == 'karen.buckley@enron.com') \\\n",
    "# #         & (df_proba['receiver'] == 'c..aucoin@enron.com')  \\\n",
    "# #         & (df_proba['likelihood'] == 0) \n",
    "#         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path_to_data + 'predictions_random.txt')\n",
    "# df_proba[df_proba['mail_id'] == 298389.0].sort_values(['likelihood'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[df_proba['mail_id'] == mail_id].sort_values(['likelihood'], ascending = False)[:10]['receiver'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in submission.iterrows():\n",
    "    mail_id = row[0]\n",
    "    receivers_id = df_proba[df_proba['mail_id'] == mail_id].\\\n",
    "        sort_values(['likelihood'], ascending = False)[:10]['receiver'].values\n",
    "    receiver_list = \" \".join(receivers_id)\n",
    "    \n",
    "    submission.loc[index, 'recipients'] = receiver_list\n",
    "submission.to_csv('submission_test.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z3</td>\n",
       "      <td>0.433911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature     tfidf\n",
       "0      z3  0.433911"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_in_doc(X_mails_cv, vectorizer.get_feature_names(), 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
