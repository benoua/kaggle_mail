{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful fns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_callback(df, mail_id, sender, receiver, word, likelihood, p1, p2, p3 ):\n",
    "    \n",
    "    df.loc[-1] = [ mail_id, sender, receiver, word, likelihood, p1, p2, p3]\n",
    "    df.index = df.index + 1\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk([list(np.arange(10))], [list( np.random.permutation(10))], k=10)\n",
    "mapk([['a', 'b']], [['b', 'c', 'd']], k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = \"Data/\"\n",
    "\n",
    "##########################\n",
    "# load some of the files #                           \n",
    "##########################\n",
    "\n",
    "training = pd.read_csv(path_to_data + 'training_set.csv', sep=',', header=0)\n",
    "\n",
    "training_info = pd.read_csv('training_info_processed.csv', sep=',', header=0)\n",
    "\n",
    "test_info = pd.read_csv('test_info_processed.csv', sep=',', header=0)\n",
    "\n",
    "test = pd.read_csv(path_to_data + 'test_set.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes :\n",
    "Careful, there is duplicates in 'mid' for training_info and test_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# create some handy structures #                    \n",
    "################################\n",
    "                            \n",
    "# convert training set to dictionary\n",
    "emails_ids_per_sender = {}\n",
    "for index, series in training.iterrows():\n",
    "    row = series.tolist()\n",
    "    sender = row[0]\n",
    "    ids = row[1:][0].split(' ')\n",
    "    emails_ids_per_sender[sender] = ids\n",
    "\n",
    "# save all unique sender names\n",
    "all_senders = emails_ids_per_sender.keys()\n",
    "\n",
    "# create address book with frequency information for each user\n",
    "address_books = {}\n",
    "i = 0\n",
    "\n",
    "if (os.path.isfile('all_users.pkl')) & (os.path.isfile('address_books.pkl')) & (os.path.isfile('all_recs.pkl')):\n",
    "    all_recs = pickle.load(open('all_recs.pkl', 'rb'))                                   \n",
    "    all_users = pickle.load(open('all_users.pkl', 'rb'))\n",
    "    address_books = pickle.load(open('address_books.pkl', 'rb'))\n",
    "else:\n",
    "    for sender, ids in emails_ids_per_sender.items():\n",
    "        recs_temp = []\n",
    "        for my_id in ids:\n",
    "\n",
    "            '''Recipients'''\n",
    "            recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "            recipients = recipients[0].split(' ')\n",
    "            # keep only legitimate email addresses\n",
    "            recipients = [rec for rec in recipients if '@' in rec]\n",
    "            recs_temp.append(recipients)\n",
    "\n",
    "\n",
    "            '''mail info'''\n",
    "\n",
    "        # flatten    \n",
    "        recs_temp = [elt for sublist in recs_temp for elt in sublist]\n",
    "        # compute recipient counts\n",
    "        rec_occ = dict(Counter(recs_temp))\n",
    "        # order by frequency\n",
    "        sorted_rec_occ = sorted(rec_occ.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        # save\n",
    "        address_books[sender] = sorted_rec_occ\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print (i)\n",
    "        i += 1\n",
    "\n",
    "    # save all unique recipient names    \n",
    "    all_recs = list(set([elt[0] for sublist in address_books.values() for elt in sublist]))\n",
    "\n",
    "    # save all unique user names \n",
    "    all_users = []\n",
    "    all_users.extend(all_senders)\n",
    "    all_users.extend(all_recs)\n",
    "    all_users = list(set(all_users))\n",
    "    \n",
    "    pickle.dump(all_recs, open('all_recs.pkl', 'wb')) \n",
    "    pickle.dump(all_users, open('all_users.pkl', 'wb')) \n",
    "    pickle.dump(address_books, open('address_books.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DG and MG\n"
     ]
    }
   ],
   "source": [
    "'''Construct the communication graph of senders/receivers'''\n",
    "\n",
    "import networkx as nx\n",
    "import pdb\n",
    "\n",
    "DG_path = 'DG.text'\n",
    "MG_path = 'MG.text'\n",
    "\n",
    "# check if it already exists\n",
    "if (os.path.isfile(DG_path)) & (os.path.isfile(MG_path)):\n",
    "    DG = pickle.load(open(DG_path))\n",
    "    MG = pickle.load(open(MG_path))\n",
    "\n",
    "else:\n",
    "    DG=nx.DiGraph()\n",
    "    MG = nx.MultiDiGraph()\n",
    "\n",
    "    for sender, ids in emails_ids_per_sender.items():\n",
    "    #     recs_temp = []\n",
    "        DG.add_node(sender)\n",
    "        MG.add_node(sender)\n",
    "        recs_temp = []\n",
    "        recipients = []\n",
    "        for my_id in ids:\n",
    "            recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "            recipients = recipients[0].split(' ')\n",
    "            # keep only legitimate email addresses\n",
    "            recipients = [rec for rec in recipients if \"@\" in rec]\n",
    "\n",
    "            DG.add_nodes_from(recipients)\n",
    "            MG.add_nodes_from(recipients)\n",
    "\n",
    "            for recipient in recipients:\n",
    "                MG.add_edge(sender, recipient, email = my_id)\n",
    "                if DG.has_edge(sender, recipient):\n",
    "                    # we added this one before, just increase the weight by one\n",
    "                    DG[sender][recipient]['weight'] += 1\n",
    "                else:\n",
    "                    # new edge. add with weight=1\n",
    "                    DG.add_edge(sender, recipient, weight = 1)\n",
    "    '''saving graphs'''\n",
    "    print(\"Saving DG and MG\")\n",
    "    pickle.dump(DG, open('DG.txt', 'wb'))\n",
    "    pickle.dump(MG, open('MG.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # from nltk import FreqDist\n",
    "\n",
    "big_string = training_info['pre_processed'].str.cat(sep=',')\n",
    "fdistr = Counter(big_string.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_all_body_column(training_info, test=False):\n",
    "    process = lambda x: pre_process_to_string(x)\n",
    "    training_info['pre_processed'] = training_info['body'].apply(process)\n",
    "    if test :\n",
    "        training_info.to_csv(\"test_info_processed.csv\")\n",
    "    else :\n",
    "        training_info.to_csv(\"training_info_processed.csv\")\n",
    "    \n",
    "# from IPython.display import Audio\n",
    "# sound_file = 'reveil.mp3'\n",
    "# Audio(url=sound_file, autoplay=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mail steming'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Mail steming'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/benlet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/benlet/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/benlet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/benlet/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('maxent_treebank_pos_tagger') # for POS tagging\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process(content):\n",
    "    # Remove formatting\n",
    "    content =  re.sub(\"\\s+\", \" \", content)\n",
    "    # Convert to lower case\n",
    "    content = content.lower()\n",
    "    \n",
    "    # Replace punctuation by space (preserving intra-word dashes)\n",
    "    content = \"\".join(letter if letter not in punct else \" \" for letter in content )\n",
    "    \n",
    "    # Remove punctuation (preserving intra-word dashes)\n",
    "    content = \"\".join(letter for letter in content if letter not in punct)\n",
    "    \n",
    "    # Remove extra white space\n",
    "    content = re.sub(\" +\",\" \", content)\n",
    "    # Remove leading and trailing white space\n",
    "    content = content.strip()\n",
    "    # Tokenize and stopword removal\n",
    "    tokens_keep  = [word for word in content.split() if word not in stpwds] \n",
    "    # POS-tag \n",
    "    tagged_tokens = nltk.pos_tag(tokens_keep)\n",
    "#     Keep only nouns and adjectives    \n",
    "    tokens_keep = [pair[0] for pair in tagged_tokens if (pair[1] in [\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\",\"JJS\",\"JJR\"])]\n",
    "    # Apply Porter stemmer\n",
    "    tokens_keep = [stemmer.stem(token) for token in tokens_keep]\n",
    "    return tokens_keep\n",
    "\n",
    "def pre_process_to_string(content):\n",
    "    return \",\".join(pre_process(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process_all_body_column(training_info)\n",
    "# process_all_body_column(test_info, test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_info.fillna(\"\", inplace = True)\n",
    "test_info.fillna(\"\", inplace = True)\n",
    "nb_training = training_info.shape[0]\n",
    "nb_test = test_info.shape[0]\n",
    "all_mails = pd.concat([training_info['pre_processed'], test_info['pre_processed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45975, 118013)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TF-IDF on emails'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all_mails = vectorizer.fit_transform(all_mails)\n",
    "X_all_mails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = np.random.permutation(nb_training)\n",
    "train_id = mask[:int(0.98*nb_training)]\n",
    "cv_id = mask[int(0.98*nb_training):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_mails_train = X_all_mails[train_id, :]\n",
    "X_mails_cv = X_all_mails[cv_id, :]\n",
    "X_mails_test = X_all_mails[nb_training:, :]\n",
    "df_train = training_info.iloc[train_id].reset_index()\n",
    "df_cv = training_info.iloc[cv_id].reset_index()\n",
    "df_test = test_info.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.decode(X_mails_train[1]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# top_idf = X_all_mails.sum(axis=0).tolist()[0]\n",
    "# top_idf_10000 = np.argpartition(top_idf, -1000)[-1000:]\n",
    "# idf_names = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_tfidf_train = pd.DataFrame(data = X_all_mails[:nb_training, top_idf_10000].todense().tolist(), \\\n",
    "#                         columns = idf_names[top_idf_10000], \\\n",
    "#                        index = training_info['mid'].values)\n",
    "\n",
    "# df_tfidf_test = pd.DataFrame(data = X_all_mails[nb_training:, top_idf_10000].todense().tolist(), \\\n",
    "#                         columns = idf_names[top_idf_10000], \\\n",
    "#                        index = test_info['mid'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# idx_test = 74\n",
    "# print(\"Memory used trained : %d MB\" %(df_tfidf_train.memory_usage().sum() / 1e6))\n",
    "# print(df_tfidf_train.loc[idx_test].sort_values(ascending = False)[:10])\n",
    "# # print(training_info.iloc[idx_test].loc['recipients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cosine_df(df1, df2, idx1, idx2):\n",
    "    return cosine_similarity(np.atleast_2d(df1.loc[idx1].values), np.atleast_2d(df2.loc[idx2].values))[0][0]\n",
    "\n",
    "def cosine_sparse(X1, X2, idx1, idx2):\n",
    "    if (X1[idx1].toarray().all() == 0) or (X2[idx2].toarray().all() == 0):\n",
    "        out = 0\n",
    "    else : \n",
    "        cosine_similarity(X1[idx1], X2[idx2])[0][0]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing likelihoods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Sender Likelihood'''\n",
    "k = 10\n",
    "# frequency based probability\n",
    "sender = 'sylvia.hu@enron.com'\n",
    "receiver = 'britt.davis@enron.com'\n",
    "\n",
    "def total_incoming_mails(receiver_):\n",
    "    return sum([DG[sender_][receiver_]['weight'] \\\n",
    "                for sender_ in DG.predecessors(receiver_)])\n",
    "\n",
    "# dictionnary of incoming mails per receiver \n",
    "dict_incoming_mails = {}\n",
    "for recipient in all_recs:\n",
    "    dict_incoming_mails[recipient] = total_incoming_mails(recipient)\n",
    "    \n",
    "\n",
    "\n",
    "def p_fred_S_sachant_R(DG, sender, receiver, k=k):\n",
    "    \n",
    "    out = DG[sender][receiver]['weight']/ \\\n",
    "                dict_incoming_mails[receiver]\n",
    "    \n",
    "#     save_callback(\"p(S/R)\", None, sender, receiver, None, out )\n",
    "    return out\n",
    "    \n",
    "# co-occurence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_fred_S_sachant_R(DG=DG, sender=sender, receiver=receiver, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load pagerank.py\n",
    "import operator\n",
    "import math, random, sys, csv \n",
    "from utils import parse, print_results\n",
    "\n",
    "class PageRank:\n",
    "    def __init__(self, graph, directed):\n",
    "        self.graph = graph\n",
    "        self.V = len(self.graph)\n",
    "        self.d = 0.85\n",
    "        self.directed = directed\n",
    "        self.ranks = dict()\n",
    "    \n",
    "    def rank(self):\n",
    "        for key, node in self.graph.nodes(data=True):\n",
    "            if self.directed:\n",
    "                self.ranks[key] = 1/float(self.V)\n",
    "            else:\n",
    "                self.ranks[key] = node.get('rank')\n",
    "\n",
    "        for _ in range(100):\n",
    "            if _%10 == 0 :\n",
    "                print(_)\n",
    "            for key, node in self.graph.nodes(data=True):\n",
    "                rank_sum = 0\n",
    "                curr_rank = node.get('rank')\n",
    "                if self.directed:\n",
    "                    neighbors = self.graph.out_edges(key)\n",
    "                    for n in neighbors:\n",
    "                        outlinks = len(self.graph.out_edges(n[1]))\n",
    "                        if outlinks > 0:\n",
    "                            rank_sum += (1 / float(outlinks)) * self.ranks[n[1]]\n",
    "                else: \n",
    "                    neighbors = self.graph[key]\n",
    "                    for n in neighbors:\n",
    "                        if self.ranks[n] is not None:\n",
    "                            outlinks = len(self.graph.neighbors(n))\n",
    "                            rank_sum += (1 / float(outlinks)) * self.ranks[n]\n",
    "            \n",
    "                # actual page rank compution\n",
    "                self.ranks[key] = ((1 - float(self.d)) * (1/float(self.V))) + self.d*rank_sum\n",
    "\n",
    "        return p\n",
    "\n",
    "# Source code https://github.com/timothyasp/PageRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Recipient Likelihood'''\n",
    "\n",
    "# number of emails received by Receiver / Total # of emails sent\n",
    "\n",
    "\n",
    "def total_mail_sent(DG=DG):\n",
    "    A = np.array(list(DG.edges_iter(data='weight', default=1)))\n",
    "    return np.sum(A[:,2:].flatten().astype(np.int),axis=0)\n",
    "\n",
    "Total_emails_sent = total_mail_sent()\n",
    "\n",
    "def p_R(DG, receiver):\n",
    "    out = dict_incoming_mails[receiver] /Total_emails_sent\n",
    "#     save_callback(\"p(R)\", None, None, receiver, None, out )\n",
    "    return out\n",
    "\n",
    "\n",
    "# PageRank\n",
    "# p = PageRank(DG, True)\n",
    "# p.rank()\n",
    "\n",
    "def p_R_pagerank(receiver):\n",
    "    return dict(p.ranks)[receiver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.6 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.2967627231646784e-05"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "p_R(DG=DG, receiver= receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p = PageRank(DG, True)\n",
    "# p.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p_R_pagerank(receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Email Likelihood'''\n",
    "\n",
    "email_id = emails_ids_per_sender[sender][0]\n",
    "email_body = training_info[training_info['mid']==int(email_id)]['pre_processed'].values[0]\n",
    "word = pre_process(email_body)[0]\n",
    "receiver = training_info[training_info['mid']==int(email_id)]['recipients'].values[0].split(\" \")[0]\n",
    "\n",
    "total_words_email = sum(list(fdistr.values()))\n",
    "\n",
    "def word_likelihood_receiver_sender(receiver, sender, word ):\n",
    "    word_occurence = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    # list all mail between sender and receiver\n",
    "#     A = [list(s.values()) for s in MG.get_edge_data(sender,receiver).values()]\n",
    "    mail_list = [ a for sublist in \\\n",
    "                 [list(s.values()) for s in MG.get_edge_data(sender,receiver).values()] for a in sublist  ]\n",
    "    \n",
    "    for emails_ids in mail_list:\n",
    "        email_body = training_info[training_info['mid']==int(emails_ids)]['pre_processed'].values[0]\n",
    "        \n",
    "        if isinstance(email_body, str):\n",
    "            email_body = email_body.split(\",\")\n",
    "        \n",
    "            word_occurence += email_body.count(word)\n",
    "            total_words += len(email_body)\n",
    "    \n",
    "    out = word_occurence / total_words\n",
    "    return out\n",
    "        \n",
    "def word_likelihood_receiver(receiver, word ):\n",
    "    word_occurence = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    A = np.array(list(MG.in_edges_iter(receiver, data=True)))[:,2:].flatten()\n",
    "    email_id_receiver = np.array([list(item.values()) for item in A ]).flatten()\n",
    "    \n",
    "    for emails_ids in email_id_receiver:\n",
    "        email_body = training_info[training_info['mid']==int(emails_ids)]['pre_processed'].values[0]\n",
    "        if isinstance(email_body, str) :\n",
    "            email_body = email_body.split(\",\")\n",
    "            word_occurence += email_body.count(word)\n",
    "            total_words += len(email_body)\n",
    "    \n",
    "    out = word_occurence / total_words\n",
    "    return out\n",
    "    \n",
    "def word_likelihood( word ):\n",
    "    word_occurence = fdistr[word]\n",
    "    \n",
    "    out = word_occurence/ total_words_email\n",
    "    return  out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.332867547403761e-05"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "word_likelihood('dear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "word_likelihood_receiver_sender(receiver, sender, 'friday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 15.6 ms, total: 31.2 ms\n",
      "Wall time: 2.24 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "word_likelihood_receiver(receiver, 'friday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# email likelihood final \n",
    "lambda_ = 0.8\n",
    "gamma_ = 0.2\n",
    "beta_ = 0.2\n",
    "sender = 'karen.buckley@enron.com'\n",
    "email_id = emails_ids_per_sender[sender][0]\n",
    "email_id = 298389 \n",
    "# email_body = training_info[training_info['mid']==int(email_id)]['body'].values[0]\n",
    "# word = pre_process(email_body)[0]\n",
    "# receiver = training_info[training_info['mid']==int(email_id)]['recipients'].values[0].split(\" \")[0]\n",
    "# receiver = test_info[test_info['mid']==int(email_id)]['recipients'].values[0].split(\" \")[0]\n",
    "\n",
    "\n",
    "def p_e_sachant_r_s(email_id, sender, receiver):\n",
    "    likelihood = 1\n",
    "    \n",
    "    email_body = test_info[test_info['mid']==int(email_id)]['pre_processed'].values\n",
    "    if email_body : \n",
    "        email_body = email_body[0]\n",
    "        if isinstance(email_body, str) :\n",
    "            email_body = email_body.split(\",\")\n",
    "            likelihood = 1\n",
    "            for (i,word) in enumerate(email_body): \n",
    "                if fdistr[word] != 0 :\n",
    "                    a1 = word_likelihood_receiver_sender(receiver, sender, word)\n",
    "#                     a2 = word_likelihood_receiver(receiver, word)\n",
    "                    a3 = word_likelihood(word)\n",
    "                    end = time.time()\n",
    "                    \n",
    "\n",
    "                    likelihood = likelihood * (lambda_*a1 + gamma_*a2 + beta_*a3)\n",
    "#                     print (\"Mot %d sur %d \" %(i, len(email_body)))\n",
    "        \n",
    "    return likelihood\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# p_e_sachant_r_s(email_id, sender, receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 289035\n",
    "mid = 288446\n",
    "if df_cv[df_cv['mid'] == mid].empty:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid = 278313\n",
    "df_cv[df_cv['mid'] == int(mid)].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Email likelihood with TF_IDF'''\n",
    "\n",
    "\n",
    "def p_e_sachant_r_s_tfidf(email_id, sender, receiver, cv_or_test= 'cv'):\n",
    "    \n",
    "    out = 1e-16\n",
    "    if MG.get_edge_data(sender,receiver): \n",
    "        '''list all mails between sender and receiver'''\n",
    "        mail_list = [ a for sublist in \\\n",
    "                     [list(s.values()) for s in MG.get_edge_data(sender,receiver).values()] for a in sublist  ]\n",
    "        \n",
    "        mail_list = [mid for mid in mail_list if df_cv[df_cv['mid'] == int(mid)].empty == True  ]\n",
    "        \n",
    "        if mail_list:\n",
    "            mail_tf_idf_scores = []\n",
    " \n",
    "            for mid in mail_list:\n",
    "                idx_train = df_train[df_train['mid'] == int(mid)].index.values[0]\n",
    "                if cv_or_test == 'test':\n",
    "                    idx_test = df_test[df_test['mid'] == int(email_id)].index.values[0]\n",
    "                    mail_tf_idf_scores.append( cosine_sparse(X_mails_train, X_mails_test, idx_train, idx_test ) )\n",
    "                else :\n",
    "                    idx_cv = df_cv[df_cv['mid'] == int(email_id)].index.values[0]\n",
    "                    mail_tf_idf_scores.append( cosine_sparse(X_mails_train, X_mails_cv, idx_train, idx_cv ) )\n",
    "\n",
    "            if mail_tf_idf_scores:\n",
    "                out = np.max(np.array(mail_tf_idf_scores))\n",
    "                if out ==0 :\n",
    "                    out = 1e-16\n",
    "#                 save_callback(\"p(E/S,R)\", email_id, sender, receiver, None, out)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30109\n"
     ]
    }
   ],
   "source": [
    "mid = 376689\n",
    "idx_train = df_train[df_train['mid'] == int(mid)].index.values[0]\n",
    "print(idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# p_e_sachant_r_s_tfidf(email_id, sender, receiver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training[training['mids'].str.contains(str(df_cv.iloc[index]['mid']))]['sender'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sender 0 over 873 calculated in 0 min\n",
      "1\n",
      "Sender 1 over 873 calculated in 0 min\n",
      "2\n",
      "Sender 2 over 873 calculated in 1 min\n",
      "3\n",
      "Sender 3 over 873 calculated in 1 min\n",
      "4\n",
      "Sender 4 over 873 calculated in 1 min\n",
      "5\n",
      "Sender 5 over 873 calculated in 1 min\n",
      "6\n",
      "Sender 6 over 873 calculated in 1 min\n",
      "7\n",
      "Sender 7 over 873 calculated in 1 min\n",
      "8\n",
      "Sender 8 over 873 calculated in 1 min\n",
      "9\n",
      "Sender 9 over 873 calculated in 2 min\n",
      "10\n",
      "Sender 10 over 873 calculated in 2 min\n",
      "11\n",
      "Sender 11 over 873 calculated in 2 min\n",
      "12\n",
      "Sender 12 over 873 calculated in 2 min\n",
      "13\n",
      "Sender 13 over 873 calculated in 2 min\n",
      "14\n",
      "Sender 14 over 873 calculated in 2 min\n",
      "15\n",
      "Sender 15 over 873 calculated in 3 min\n",
      "16\n",
      "Sender 16 over 873 calculated in 3 min\n",
      "17\n",
      "Sender 17 over 873 calculated in 3 min\n",
      "18\n",
      "Sender 18 over 873 calculated in 3 min\n",
      "19\n",
      "Sender 19 over 873 calculated in 3 min\n",
      "20\n",
      "Sender 20 over 873 calculated in 3 min\n",
      "21\n",
      "Sender 21 over 873 calculated in 3 min\n",
      "22\n",
      "Sender 22 over 873 calculated in 4 min\n",
      "23\n",
      "Sender 23 over 873 calculated in 4 min\n",
      "24\n",
      "Sender 24 over 873 calculated in 4 min\n",
      "25\n",
      "Sender 25 over 873 calculated in 4 min\n",
      "26\n",
      "Sender 26 over 873 calculated in 4 min\n",
      "27\n",
      "Sender 27 over 873 calculated in 4 min\n",
      "28\n",
      "Sender 28 over 873 calculated in 4 min\n",
      "29\n",
      "Sender 29 over 873 calculated in 4 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "df_proba_cv =pd.DataFrame(columns=[\"mail_id\",\"sender\", \"receiver\",\"word\", \"likelihood\",'P(E/R,S)', 'P(R/S)', 'P(R)'])\n",
    "start_time = time.time()\n",
    "for index, row in df_cv[:30].iterrows():\n",
    "    print(index)\n",
    "    mail_id = df_cv.loc[index]['mid']\n",
    "    sender = training[training['mids'].str.contains(str(mail_id))]['sender'].values[0]\n",
    "    \n",
    "    receiver_count = 0\n",
    "    for (receiver, _) in address_books[sender][:300]:\n",
    "        \n",
    "\n",
    "        a2 = p_fred_S_sachant_R(DG, sender, receiver)\n",
    "        a3 = p_R(DG, receiver)\n",
    "        a1 = p_e_sachant_r_s_tfidf(mail_id,sender,receiver)\n",
    "        out = a1 * a2 * a3 \n",
    "        save_callback( df_proba_cv, mail_id, sender, receiver, None, out, a1, a2, a3)\n",
    "    receiver_count += 1\n",
    "    print (\"Sender %d over %d calculated in %d min\" % (index, len(df_cv), (time.time() - start_time)/60 ))\n",
    "\n",
    "df_proba_cv.to_csv(\"df_proba_cv.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f404743a4e0>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f404743bb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_proba_cv['P(E/R,S)'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_fred_S_sachant_R(DG, 'lorna.brennan@enron.com', 'larry.pavlou@enron.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_proba_cv.to_csv(\"df_proba_cv.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_receivers = 10\n",
    "def f(weights = [0,1,1]):\n",
    "    mapk_predicted =[]\n",
    "    mapk_true = []\n",
    "    mapk_index = []\n",
    "\n",
    "    for index, row in df_cv[:30].iterrows():\n",
    "        mid_cv = row[3]\n",
    "    #     print(mid_cv)\n",
    "        df_pow = df_proba_cv[df_proba_cv['mail_id'] == mid_cv][['P(E/R,S)', 'P(R/S)', 'P(R)']].pow(weights)\n",
    "\n",
    "        df_pow['likelihood'] = df_pow.prod(axis = 1)\n",
    "        df_pow = df_pow.sort_values(by='likelihood', ascending = False)\n",
    "        receiver_names = df_proba_cv.loc[df_pow.index][:top_receivers]['receiver'].values\n",
    "    #     receiver_list_predicted = \" \".join(receiver_names)\n",
    "        receiver_list_predicted = receiver_names\n",
    "\n",
    "        receiver_list_true = training_info[training_info['mid'] ==mid_cv]['recipients'].values[0].split(\" \")\n",
    "#         print(receiver_list_predicted[:top_receivers])\n",
    "#         print(\"######\")\n",
    "#         print(receiver_list_true[:top_receivers])\n",
    "#         print(apk(receiver_list_true,receiver_list_predicted, k=10))\n",
    "        mapk_index.append(mid_cv)\n",
    "        mapk_predicted.append(receiver_list_predicted)\n",
    "        mapk_true.append(receiver_list_true)\n",
    "    \n",
    "    return -mapk(mapk_true, mapk_predicted, k=10) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "weights_ini  = [0.5, 10, 1]\n",
    "best_weights = minimize(f, weights_ini, method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.216199074074\n",
      "[ 0.51666667  9.5         1.03333333]\n"
     ]
    }
   ],
   "source": [
    "print(- f(best_weights['x']))\n",
    "print (best_weights['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41101851851851851"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- f([1, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[ 0.51666667,  9.5       ,  1.03333333],\n",
       "       [ 0.51666463,  9.50006104,  1.03333537],\n",
       "       [ 0.51666294,  9.50002035,  1.03333605],\n",
       "       [ 0.5166648 ,  9.50001017,  1.03333774]]), array([-0.21619907, -0.21619907, -0.21619907, -0.21619907]))\n",
       "           fun: -0.21619907407407407\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 74\n",
       "           nit: 17\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([ 0.51666667,  9.5       ,  1.03333333])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for index, row in test.iterrows():\n",
    "    name_ids = row.tolist()\n",
    "    sender = name_ids[0]\n",
    "    # get IDs of the emails for which recipient prediction is needed\n",
    "    ids_predict = name_ids[1].split(' ')\n",
    "    ids_predict = [int(my_id) for my_id in ids_predict]\n",
    "    \n",
    "    receiver_count = 0\n",
    "    for (receiver, _) in address_books[sender]:\n",
    "        receiver_count += 1\n",
    "#         print (\"Receiver %d over 50 most used\" %(receiver_count))\n",
    "        if _ > 1 : # so as to limit algorithm \n",
    "        \n",
    "            a2 = p_fred_S_sachant_R(DG=DG,sender=sender,receiver=receiver)\n",
    "            a3 = p_R(DG,receiver=receiver)\n",
    "            for mail_id in ids_predict:\n",
    "                a1 = 1\n",
    "#                 start = time.time()\n",
    "                a1 = p_e_sachant_r_s_tfidf(mail_id,sender,receiver)\n",
    "#                 stop = time.time()\n",
    "#                 print (stop-start)\n",
    "\n",
    "                out = (a1**(1/3)) * a2 * a3 \n",
    "\n",
    "                save_callback(\"p(R/S,E)\", mail_id, sender, receiver, None, out)\n",
    "    \n",
    "    print (\"Sender %d over %d calculated in %d min\" % (index, len(test), (time.time() - start_time)/60 ))\n",
    "\n",
    "df_proba.to_csv(\"df_proba.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "# sound_file = 'reveil.mp3'\n",
    "# Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[(df_proba['proba_id'] == 'p(R/S,E)') & (df_proba['sender'] =='ginger.dernehl@enron.com')].sort_values(by= 'likelihood' , ascending = False)\n",
    "df_proba = df_proba.drop(df_proba[(df_proba['receiver'] == df_proba['sender']) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.570375601666\n",
      "0.00105563375065\n",
      "0.2912280701754386\n"
     ]
    }
   ],
   "source": [
    "print(p_e_sachant_r_s_tfidf(298389, 'karen.buckley@enron.com', 'scott.neal@enron.com'))\n",
    "print (p_R(DG,'scott.neal@enron.com'))\n",
    "print (p_fred_S_sachant_R(DG, 'karen.buckley@enron.com', 'scott.neal@enron.com' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.570375601666\n",
      "0.00105563375065\n",
      "0.2912280701754386\n"
     ]
    }
   ],
   "source": [
    "print(p_e_sachant_r_s_tfidf(298389.0, 'karen.buckley@enron.com', 'scott.neal@enron.com'))\n",
    "print (p_R(DG,'scott.neal@enron.com'))\n",
    "print (p_fred_S_sachant_R(DG, 'karen.buckley@enron.com', 'scott.neal@enron.com' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[df_proba['mail_id'] == 298389.0].sort_values(['likelihood'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_proba[(df_proba['mail_id'] == 298389)  \\\n",
    "# #         & (df_proba['sender'] == 'karen.buckley@enron.com') \\\n",
    "# #         & (df_proba['receiver'] == 'c..aucoin@enron.com')  \\\n",
    "# #         & (df_proba['likelihood'] == 0) \n",
    "#         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path_to_data + 'predictions_random.txt')\n",
    "# df_proba[df_proba['mail_id'] == 298389.0].sort_values(['likelihood'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['richard.shapiro@enron.com', 'mark.whitt@enron.com',\n",
       "       'steven.j.kean@enron.com', 'shelley.corman@enron.com',\n",
       "       'rick.buy@enron.com', 'jarnold@enron.com',\n",
       "       'kimberly.watson@enron.com', 'e..haedicke@enron.com',\n",
       "       'jshankm@enron.com', 'jsteffe@enron.com'], dtype=object)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proba[df_proba['mail_id'] == mail_id].sort_values(['likelihood'], ascending = False)[:10]['receiver'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chris.mahoney@enron.com anabel.soria@travelpark.com wk@transcarriers.com mpatterson@testmail.ercot.com expense.report@enron.com gateway1@pdq.net glwaas@calpx.com mitra.mujica@enron.com hjreed@powersrc.com brian.wesneske@enron.com'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.ix[0]['recipients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in submission.iterrows():\n",
    "    mail_id = row[0]\n",
    "    receivers_id = df_proba[df_proba['mail_id'] == mail_id].\\\n",
    "        sort_values(['likelihood'], ascending = False)[:10]['receiver'].values\n",
    "    receiver_list = \" \".join(receivers_id)\n",
    "    \n",
    "    submission.loc[index, 'recipients'] = receiver_list\n",
    "submission.to_csv('submission_test.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z3</td>\n",
       "      <td>0.433911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature     tfidf\n",
       "0      z3  0.433911"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_in_doc(X_mails_cv, vectorizer.get_feature_names(), 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-766e7d386cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m199773\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0midx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m199773\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "mid = 199773\n",
    "idx_train = df_train[df_train['mid'] == int(199773)].index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_cv = df_cv[df_cv['mid'] == int(mid)].index.values[0]\n",
    "idx_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv[df_cv['mid'] == int(mid)].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>mid</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>recipients</th>\n",
       "      <th>pre_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23066</th>\n",
       "      <td>23066</td>\n",
       "      <td>23066</td>\n",
       "      <td>199773</td>\n",
       "      <td>2000-09-21 02:27:00</td>\n",
       "      <td>Note:  Ken Lay s comments at the \"Governors  N...</td>\n",
       "      <td>dave.neubauer@enron.com chuck.wilkinson@enron....</td>\n",
       "      <td>note,lay,comment,governor,natur,ga,summit,ener...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1     mid                 date  \\\n",
       "23066       23066         23066  199773  2000-09-21 02:27:00   \n",
       "\n",
       "                                                    body  \\\n",
       "23066  Note:  Ken Lay s comments at the \"Governors  N...   \n",
       "\n",
       "                                              recipients  \\\n",
       "23066  dave.neubauer@enron.com chuck.wilkinson@enron....   \n",
       "\n",
       "                                           pre_processed  \n",
       "23066  note,lay,comment,governor,natur,ga,summit,ener...  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_info[training_info['mid'] == mid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
